{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d61843-a42a-4b55-a99b-1a7d9b359e1d",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Будет проводить на основе выводов полученных из EDA. Включает в себя и Data Cleaning, поскольку избавиться от неинформативных признаков так же важно, как и сконструировать новые. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a925ba5-1072-4b94-98ff-f05b8c2103f9",
   "metadata": {},
   "source": [
    "## Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7110f4-821f-4529-8916-befde122b958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules were imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                      StratifiedKFold, \n",
    "                                      RandomizedSearchCV)\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, \n",
    "                              recall_score, roc_curve, confusion_matrix)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "print(\"All modules were imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff90655-7343-47bd-a6bf-c532292205dc",
   "metadata": {},
   "source": [
    "### Полезные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0762f880-28d8-4e50-8d28-7511130bdf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data, verbose = True):\n",
    "    \"\"\"\n",
    "    Optimizes memory usage of numeric columns in a DataFrame by downcasting their data types.\n",
    "    \n",
    "    The function analyzes value ranges in numeric columns and converts them to the smallest \n",
    "    possible data types while preserving all information. This significantly reduces \n",
    "    memory usage without data loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        Input DataFrame to optimize. Modified in-place.\n",
    "    verbose : bool, default=True\n",
    "        If True, prints detailed optimization process information:\n",
    "        - Initial memory usage\n",
    "        - List of optimized columns with old and new types\n",
    "        - Final memory usage\n",
    "        - Percentage of memory reduction\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Optimized DataFrame with reduced memory consumption.\n",
    "        Returns the same object as input (modified in-place).\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Only numeric columns (int and float types) are optimized\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    initial_mem = data.memory_usage().sum()/1024**2\n",
    "    if verbose:\n",
    "        print(\"Initial mem usage {.:2B}\".format(initial_mem))\n",
    "\n",
    "    int_limits = {\n",
    "        np.int8 : (np.iinfo(np.int8).min, np.iinfo(np.int8).max), \n",
    "        np.int16 : (np.iinfo(np.int16).min, np.iinfo(np.int16).max),\n",
    "        np.int32 : (np.iinfo(np.int32).min, np.iinfo(np.int32).max),\n",
    "        np.int64 : (np.iinfo(np.int64).min, np.iinfo(np.int64).max)}\n",
    "\n",
    "    float_limits = {\n",
    "        np.float8 : (np.finfo(np.float8).min, np.finfo(np.float8).max), \n",
    "        np.float16 : (np.finfo(np.float16).min, np.finfo(np.float16).max), \n",
    "        np.float32 : (np.finfo(np.float32).min, np.finfo(np.float32).max), \n",
    "        np.float64 : (np.finfo(np.float64).min, np.finfo(np.float64).max)}\n",
    "\n",
    "    optimized_cols = []\n",
    "\n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "\n",
    "            if np.issubdtype(col_type, np.integer):\n",
    "                if int_limits[np.int8][0] <= c_min and c_max <= int_limits[np.int8][1]:\n",
    "                    new_type = np.int8\n",
    "                elif int_limits[np.int16][0] <= c_min and c_max <= int_limits[np.int16][1]:\n",
    "                    new_type = np.int16\n",
    "                elif int_limits[np.int32][0] <= c_min and c_max <= int_limits[np.int32][1]:\n",
    "                    new_type = np.int32\n",
    "                else:\n",
    "                    new_type = np.int64\n",
    "\n",
    "            else:\n",
    "                if float_limits[np.float8][0] <= c_min and c_max <= float_limits[np.float8][1]:\n",
    "                    new_type = np.float8\n",
    "                elif float_limits[np.float16][0] <= c_min and c_max <= float_limits[np.float16][1]:\n",
    "                    new_type = np.float16\n",
    "                elif float_limits[np.float32][0] <= c_min and c_max <= float_limits[np.float32][1]:\n",
    "                    new_type = np.float32\n",
    "                else:\n",
    "                    new_type = np.float64\n",
    "\n",
    "            if col_type != new_type:\n",
    "                data[col] = data[col].astype(new_type)\n",
    "                optimized_cols.append((col, str(col_type), str(new_type)))\n",
    "\n",
    "    end_mem = data.memory_usage()/1024**2\n",
    "\n",
    "    if verbose:\n",
    "        if optimized_cols:\n",
    "            print(\"Optimized columns:\")\n",
    "            for col, old_type, new_type in optimized_cols:\n",
    "                print(f\"  {col}: {old_type} -> {new_type}\")\n",
    "    \n",
    "        print(f'Memory after optimization: {end_mem:.2f} MB')\n",
    "        reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f'Reduced by {reduction:.1f}%')\n",
    "        print('-' * 80)\n",
    "    \n",
    "    return data\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fb59f2-5456-4152-9746-9d99de6987fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relational_tables_prepare(file_directory = '', verbose = True, tables = None):\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Start merging tables\")\n",
    "        start = datetime.now()\n",
    "\n",
    "    for table in tables:\n",
    "        with open(file_directory + str(table) + '_preprocessed.pkl' , \"rb\") as file:\n",
    "            table_to_merge = reduce_mem_usage(pickle.load(file), verbose = False)\n",
    "\n",
    "    with open(file_directory + 'application_train_preprocessed.pkl', 'rb') as file:\n",
    "        application_train = reduce_mem_usage(pickle.load(file), verbose = False)\n",
    "\n",
    "    with open(file_directory + 'application_test_preprocessed.pkl', 'rb') as file:\n",
    "        application_test = reduce_mem_usage(pickle.load(file), verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6dac1b-76f0-42f3-8863-56b383474d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelling:\n",
    "    '''\n",
    "    Class for Doing Hyperparameter tuning to find best set of hyperparameters, building models on best hyperparams and\n",
    "    displaying results on best hyperparameters.\n",
    "    \n",
    "    It has 4 methods:\n",
    "        1. init method\n",
    "        2. random_search_cv method\n",
    "        3. train_on_best_params method\n",
    "        4. proba_to_class method\n",
    "        5. tune_threshold method\n",
    "        6. results_on_best_params method\n",
    "        7. feat_importances_show method\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_model, x_train, y_train, x_test, calibration = False, calibration_method = 'isotonic', \n",
    "                 calibration_cv = 4, k_folds = 4, random_state = 982):\n",
    "        '''\n",
    "        Function to initialize the class members.\n",
    "        \n",
    "        Inputs: \n",
    "            self\n",
    "            base_model: estimator/classifier\n",
    "                The base model to be used for the modelling purpose\n",
    "            x_train: numpy array\n",
    "                Training standardized data\n",
    "            y_train: numpy array\n",
    "                Training class labels\n",
    "            x_test: numpy array\n",
    "                Test standardized data\n",
    "            calibration: bool, default = False\n",
    "                Whether to calibrate the model for generating class probabilities\n",
    "            calibration_method: str, default = 'isotonic'\n",
    "                The type of calibration to use, i.e. sigmoid or isotonic\n",
    "            calibration_cv: int, default = 4\n",
    "                Number of cross-validation folds for calibrating the probabilities\n",
    "            k_folds: int, default = 4\n",
    "                Number of cross-validation folds for training and tuning the model\n",
    "            random_state: int, default = 982\n",
    "                Random state for StratifiedKFold for reproducibility\n",
    "                \n",
    "        Returns: \n",
    "            None      \n",
    "        '''\n",
    "        self.base_model = base_model\n",
    "        self.num_folds = k_folds\n",
    "        self.kfolds = StratifiedKFold(n_splits = k_folds, shuffle = True, random_state = random_state)\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.calibration = calibration\n",
    "        if self.calibration:\n",
    "            self.calibration_method = calibration_method\n",
    "            self.calibration_cv = calibration_cv\n",
    "\n",
    "    def random_search(self, hyperparams_dict, n_iter = 30, verbose = True, n_jobs = 1, random_State = 843):\n",
    "\n",
    "        '''\n",
    "        Function to do RandomizedSearchCV on training data.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            hyperparams_dict: dict\n",
    "                Dictionary of hyperparameters to tune\n",
    "            n_iter: int, default = 30\n",
    "                Number of iterations to perform for random search\n",
    "            verbose: bool, default = True\n",
    "                Whether to keep verbosity or not\n",
    "            n_jobs: int, default = 1\n",
    "                Number of cores to use for Random Search\n",
    "            random_state: int, default = 843\n",
    "                Random state for reproducibility of RandomizedSearchCV\n",
    "                \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        if verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"Start doing Randomized Search CV with {n_iter} random initializations\".format(n_iter))\n",
    "        rscv = RandomizedSearchCV(self.base_model, hyperparams_dict, n_iter = n_iter, scoring = 'roc-auc',\n",
    "                                  cv = self.kfolds, return_train_score = True, verbose = 2, n_jobs = n_jobs, random_state = random_state)\n",
    "        rcsv.fit(self.x_train, self.y_train)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Done\")\n",
    "            print(f'Time elapsed = {datetime.now() - start}')\n",
    "\n",
    "        self.tuning_results = pd.DataFrame(rcsv.cv_results_)\n",
    "        self.best_model = rcsv.best_estimator_\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    def train_on_best_params(self, verbose = True):\n",
    "        '''\n",
    "        Function to train the model on best hyperparameters obtained from previous method.\n",
    "        Generates Cross-Validation predictions as Out-of-fold predictions\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            verbose: bool, default = True\n",
    "                Whether to keep verbosity or not\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "            \n",
    "        '''\n",
    "        if verbose:\n",
    "            start = datetime.now()\n",
    "            print(f\"{self.num_folds} - Fold Cross Validation\")\n",
    "            print(\"Fitting the model on best hyperparams...\")\n",
    "\n",
    "        self.cv_preds_probas = np.zeros(self.x_train.shape[0])\n",
    "        self.best_threshold_train = 0\n",
    "\n",
    "        for fold_number, (train_indices, val_indices) in enumerate(self.kfolds.split(self.x_train, self.y_train), 1):\n",
    "            if verbose:\n",
    "                print(f\"Fitting Fold {fold_number}...\")\n",
    "\n",
    "                self.best_model.fit(self.x_train[train_indices], self.y_train[train_indices])\n",
    "                if not self.calibration:\n",
    "                    self.train_preds_probas = self.best_model.predict_proba(self.x_train[train_indices])[:, 1]\n",
    "                    self.cv_preds_probas[val_indices] = self.best_model.predict_proba(self.x_train[val_indices])[:, 1]\n",
    "\n",
    "                else:\n",
    "                    self.calibrated_classifier = CalibratedClassifierCV(self.best_model, method = self.calibration_method, \n",
    "                                                                        cv = self.calibration_cv)\n",
    "                    self.calibrates_classifier.fit(self.x_train[train_indices], self.y_train[train_indices])\n",
    "                    self.train_preds_probas = self.calibrated_classifier.predict_proba(self.x_train[train_indices])[:, 1]\n",
    "                    self.cv_preds_probas[val_indices] = self.best_model.predict_proba(self.x_train[val_indices])[:, 1]\n",
    "\n",
    "                self.best_threshold_train += self.tuning_threshold(self.y_train[train_indices], self.train_preds_probas)/self.num_folds\n",
    "                \n",
    "\n",
    "        self.cv_preds_class = self.proba_to_class(self.cv_preds_probas, self.best_threshold_train)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Done\")\n",
    "            print(f\"Time elapsed = {datetime.now() - start}\")\n",
    "        gc.collect()\n",
    "\n",
    "    def proba_to_class(self, proba, threshold):\n",
    "        '''\n",
    "        Function to convert a given probability to class label based on a threshold value.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            proba: numpy array\n",
    "                Probabilities of class label = 1\n",
    "            threshold: int\n",
    "                Threshold probability to be considered as Positive or Negative Class Label\n",
    "            \n",
    "        Returns:\n",
    "            Converted Class Label\n",
    "        '''\n",
    "        return np.where(proba >= threshold, 1, 0)\n",
    "\n",
    "    def tune_threshold(self, true_labels, predicted_probas):\n",
    "        '''\n",
    "        Function to find the optimal threshold for maximizing the TPR and minimizing the FPR from ROC-AUC Curve.\n",
    "        This is found out by using the J Statistic, which is J = TPR - FPR.\n",
    "        Reference: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            true_labels: numpy array or pandas series\n",
    "                True Class Labels\n",
    "            predicted_probas: numpy array\n",
    "                Predicted Probability of Positive Class label\n",
    "            \n",
    "        Returns:\n",
    "            Threshold probability.\n",
    "        '''\n",
    "        fpr, tpr, threshold = roc_curve(true_labels, predicted_probas)\n",
    "        j_stat = tpr - fpr\n",
    "\n",
    "        best_index = np.argmax(j_stat)\n",
    "\n",
    "        return threshold[best_index]\n",
    "\n",
    "    def results_on_best_params(self, model_name):\n",
    "        '''\n",
    "        Function to train the whole data on best parameters and display the results.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            model_name: str\n",
    "                model name to get feature importances.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        self.best_model.fit(self.x_train, self.y_train)\n",
    "        if not self.calibration:\n",
    "            self.train_preds_probas = self.best_model.predict_proba(self.x_train)[:, 1]\n",
    "            self.test_preds_probas = self.best_model.predict_proba(self.x_test)[:, 1]\n",
    "        else:\n",
    "            self.calibrated_classifier.fit(self.x_train, self.y_train)\n",
    "            self.train_preds_probas = self.calibrated_classifier.predict_proba(self.x_train)[:, 1]\n",
    "            self.test_preds_probas = self.calibrated_classifier.predict_proba(self.x_test)[:, 1]\n",
    "\n",
    "        self.train_preds_class = self.proba_to_class(self.train_preds_probas, self.best_threshold_train)\n",
    "        self.test_preds_class = self.proba_to_class(self.test_preds_probas, self.best_threshold_train)\n",
    "\n",
    "        if mode_name == 'linear':\n",
    "            self.feat_imp = self.best_model.coef_[0]\n",
    "        else:\n",
    "            self.feat_impt = self.best_model.feature_importances_\n",
    "\n",
    "        print(\"-\"*100)\n",
    "        print(f\"\\nBest threshold (using j-stat) : {self.best_threshold_train}\")\n",
    "        print(\"Training results\")\n",
    "        print(f\"\\tROC-AUC score : {roc_auc_score(self.y_train, self.train_preds_probas)}\")\n",
    "        print(f\"\\tPrecision score : {precision_score(self.y_train, self.train_preds_class)}\")\n",
    "        print(f\"\\tRecall score : {recall_score(self.y_train, self.train_preds_class)}\")\n",
    "        print(\"CV results\")\n",
    "        print(f\"\\tROC-AUC score : {roc_auc_score(self.y_train, self.cv_preds_probas)}\")\n",
    "        print(f\"\\tPrecision score : {precision_score(self.y_train, self.cv_preds_class)}\")\n",
    "        print(f\"\\tRecall score : {recall_Score(self.y_train, self.cv_preds_class)}\")\n",
    "\n",
    "        pass\n",
    "\n",
    "    def feat_importances(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d25666-0d34-402e-b5fb-8dcdc3b6cd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac6915-3ed3-49ba-afab-8afdc2ef73cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6f78f-c5e9-4681-bce4-432c300a18c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf5324e-a60d-448d-bff9-11a21c08831d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f78cd-1d81-48a9-8238-7f28408bf6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
